{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5XCGKHJPg9i",
        "outputId": "df46b342-f085-47b4-925b-7ab5812b6de9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCAy1yxMmhgv"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import nltk\n",
        "import os\n",
        "import re #regular exprtion لتنضيف الداتا\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, GRU, LSTM, Bidirectional\n",
        "#from keras.layers.embeddings import Embedding\n",
        "from keras.initializers import Constant\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-OKMdPRnQW7",
        "outputId": "f3d277f2-0788-40fc-de4d-cadea6d9d534"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       is_sarcastic                                           headline\n",
            "0                 1  @USER @USER @USER I don't get this .. obviousl...\n",
            "1                 1  @USER @USER trying to protest about . Talking ...\n",
            "2                 1  @USER @USER @USER He makes an insane about of ...\n",
            "3                 1  @USER @USER Meanwhile Trump won't even release...\n",
            "4                 1  @USER @USER Pretty Sure the Anti-Lincoln Crowd...\n",
            "...             ...                                                ...\n",
            "28614             1       jews to celebrate rosh hashasha or something\n",
            "28615             1  internal affairs investigator disappointed con...\n",
            "28616             0  the most beautiful acceptance speech this week...\n",
            "28617             1  mars probe destroyed by orbiting spielberg-gat...\n",
            "28618             1                 dad clarifies this not a food stop\n",
            "\n",
            "[33619 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "data_1 = pd.read_json(\"/content/drive/MyDrive/NLP_Project/sarcasm_detection_shared_task_twitter_training.json\", lines=True)\n",
        "del data_1[\"context\"]\n",
        "#data_2 = pd.read_json(\"/content/Sarcasm_Headlines_Dataset.json\", lines=True)\n",
        "#del data_2[\"article_link\"]\n",
        "data_3 = pd.read_json(\"/content/drive/MyDrive/NLP_Project/Sarcasm_Headlines_Dataset_v2.json\", lines=True)\n",
        "del data_3[\"article_link\"]\n",
        "data =  pd.concat([data_1,data_3])\n",
        "data.head()\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSnTgkT8CcN_"
      },
      "source": [
        "# Pre Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9Vloo1LqP8L"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+') # this regular expression pattern can be used to identify and extract URLs from text strings.\n",
        "    text = pattern.sub('', text)\n",
        "    text = \" \".join(filter(lambda x:x[0]!='@', text.split()))\n",
        "    emoji = re.compile(\"[\"  # used to identify and remove Unicode emojis from a string of text\n",
        "                           u\"\\U0001F600-\\U0001FFFF\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"  #Miscellaneous symbols\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    \n",
        "    text = emoji.sub(r'', text)\n",
        "    text = text.lower()\n",
        "    # Limitation and stemming \n",
        "    # convert \" ' \" to actual word\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)        \n",
        "    text = re.sub(r\"what's\", \"what is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text) \n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)  \n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)  \n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"don't\", \"do not\", text)\n",
        "    text = re.sub(r\"did't\", \"did not\", text)\n",
        "    text = re.sub(r\"can't\", \"can not\", text)\n",
        "    text = re.sub(r\"it's\", \"it is\", text)\n",
        "    text = re.sub(r\"couldn't\", \"could not\", text)\n",
        "    text = re.sub(r\"have't\", \"have not\", text)\n",
        "    text = re.sub(r\"[,.\\\"\\'!@#$%^&*(){}?/;`~:<>+=-]\", \"\", text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_L9EZAKiqURU",
        "outputId": "57fa4db5-d3f7-43bc-c10f-327179e40209"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "<class 'list'>\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def CleanTokenize(df):\n",
        "    head_lines = list()\n",
        "    lines = df[\"headline\"].values.tolist()\n",
        "    #lines = data[\"headline\"]\n",
        "    #a=pd.array(lines, dtype=\"string\")\n",
        "    for line in lines:\n",
        "        line = clean_text(line)\n",
        "        # tokenize the text\n",
        "        tokens = word_tokenize(line)\n",
        "        # remove puntuations\n",
        "        table = str.maketrans('', '', string.punctuation)#function call creates a translation table maps each character \n",
        "        stripped = [w.translate(table) for w in tokens]#The translate() method is then called on each token in the tokens list to apply the translation table and remove punctuation\n",
        "        # remove non alphabetic characters\n",
        "        words = [word for word in stripped if word.isalpha()]\n",
        "        stop_words = set(stopwords.words(\"english\"))#This function returns a list of common English stop words that are often removed from text data in NLP tasks.\n",
        "        # remove stop words\n",
        "        words = [w for w in words if not w in stop_words]\n",
        "        head_lines.append(words)\n",
        "    return head_lines\n",
        "\n",
        "head_lines = CleanTokenize(data) #store the cleaned and tokenized headlines in the head_lines variable\n",
        "head_lines[0:1]\n",
        "print(type(head_lines))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2AxrLSTqfmZ",
        "outputId": "51f00087-6233-410b-866c-238e38096a94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "unique tokens -  33331\n",
            "vocab size - 33332\n"
          ]
        }
      ],
      "source": [
        "validation_split = 0.2  #sets the fraction of the data that will be used for validation\n",
        "max_length = 25         #sets the maximum length of the input sequences\n",
        "tokenizer_obj = Tokenizer() #initializes a Tokenizer object.\n",
        "tokenizer_obj.fit_on_texts(head_lines) # fits the Tokenizer object on the tokenized and cleaned text data. \n",
        "sequences = tokenizer_obj.texts_to_sequences(head_lines) #converts the tokenized sequences to numerical sequences using the fitted Tokenizer object.\n",
        "\n",
        "word_index = tokenizer_obj.word_index  #The word_index dictionary contains key-value pairs where each word in the text data is a key and its corresponding integer index is the value.\n",
        "print(\"unique tokens - \",len(word_index)) #The len(word_index) function returns the number of unique words in the text data.\n",
        "vocab_size = len(tokenizer_obj.word_index) + 1 #The vocab_size variable is initialized to the number of unique words in the text data plus one. This is because the integer index starts from 1 and not 0. The vocab_size indicates the size of the vocabulary that will be used to train the machine learning model.\n",
        "print('vocab size -', vocab_size)\n",
        "\n",
        "lines_pad = pad_sequences(sequences, maxlen=max_length, padding='post') #pad_sequences function is used to pad the sequences to a fixed length of max_length. The sequences variable contains the tokenized sequences for each text headline. padding='post' means that the padding will be added to the end of each sequence.\n",
        "sentiment =  data['is_sarcastic'].values #extracts the labels for each text from the data object and assigns them to the sentiment variable. This creates a NumPy array containing the sentiment labels for all the headlines in the dataset.\n",
        "\n",
        "indices = np.arange(lines_pad.shape[0]) #creates an array of indices from 0 to the number of rows in the lines_pad array\n",
        "np.random.shuffle(indices) #shuffles the indices randomly\n",
        "lines_pad = lines_pad[indices] #use the shuffled indices to rearrange the order of the rows in the lines_pad and sentiment arrays, so that the rows are in a different order than they were before.\n",
        "sentiment = sentiment[indices]\n",
        "\n",
        "num_validation_samples = int(validation_split * lines_pad.shape[0])# stores the number of validation samples, which is calculated by multiplying the validation split value with the total number of padded sequences.\n",
        "\n",
        "X_train_pad = lines_pad[:-num_validation_samples]\n",
        "y_train = sentiment[:-num_validation_samples]\n",
        "X_test_pad = lines_pad[-num_validation_samples:]\n",
        "y_test = sentiment[-num_validation_samples:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHZ7iOmLCs62",
        "outputId": "f6a39ed9-3efe-443d-d3e5-e6f5cb8d96ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X_train_pad: (26896, 25)\n",
            "Shape of y_train: (26896,)\n",
            "Shape of X_test_pad: (6723, 25)\n",
            "Shape of y_test: (6723,)\n"
          ]
        }
      ],
      "source": [
        "print('Shape of X_train_pad:', X_train_pad.shape)\n",
        "print('Shape of y_train:', y_train.shape)\n",
        "print('Shape of X_test_pad:', X_test_pad.shape)\n",
        "print('Shape of y_test:', y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAFTPrJzC6fc",
        "outputId": "bd54af88-fc51-40fb-b365-ed4438cc20dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 4907 word vectors.\n"
          ]
        }
      ],
      "source": [
        "embeddings_index = {}\n",
        "embedding_dim = 100\n",
        "GLOVE_DIR = \"/content/drive/MyDrive/NLP_Project/glove.twitter.27B.100d.txt\"\n",
        "f = open(GLOVE_DIR, encoding = \"utf-8\")\n",
        "for line in f:\n",
        "    values = line.split() #splitting each line into a list of values.\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Found %s word vectors.' % len(embeddings_index)) #prints the number of word vectors that were found in the file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEePpsTaWPCw",
        "outputId": "ae24e78e-f873-49a7-dc5e-51b5f51e3ab3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2292\n"
          ]
        }
      ],
      "source": [
        "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim)) #len(word_index) is the total number of unique words in the training data, and embedding_dim is the dimension of the pre-trained GloVe word vectors.\n",
        "c = 0\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        c+=1\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "print(c) #The variable c keeps track of the number of words for which the embedding vectors are found in the embeddings_index dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftref7mfWXKY"
      },
      "outputs": [],
      "source": [
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            embedding_dim,#the size of the embedding vector.\n",
        "                            weights=[embedding_matrix],#the pre-trained embedding matrix.\n",
        "                            input_length=max_length,#the length of the input sequences.\n",
        "                            trainable=False)#the weights of the embedding layer are frozen and will not be updated during training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKd-FDmRWa6H",
        "outputId": "94e03805-3c2e-45ba-fc1a-8bf7ab1d64ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summary of the built model...\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 25, 100)           3333200   \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 64)                42240     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,375,505\n",
            "Trainable params: 42,305\n",
            "Non-trainable params: 3,333,200\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.25))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "print('Summary of the built model...')\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zVdV1VIWes9",
        "outputId": "1dcc8f68-6041-4fef-e35a-2b5d7707c922"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "841/841 - 34s - loss: 0.6445 - acc: 0.6271 - val_loss: 0.6127 - val_acc: 0.6664 - 34s/epoch - 40ms/step\n",
            "Epoch 2/25\n",
            "841/841 - 30s - loss: 0.6060 - acc: 0.6657 - val_loss: 0.5852 - val_acc: 0.6836 - 30s/epoch - 35ms/step\n",
            "Epoch 3/25\n",
            "841/841 - 30s - loss: 0.5858 - acc: 0.6824 - val_loss: 0.5816 - val_acc: 0.6853 - 30s/epoch - 35ms/step\n",
            "Epoch 4/25\n",
            "841/841 - 29s - loss: 0.5707 - acc: 0.6939 - val_loss: 0.5644 - val_acc: 0.6921 - 29s/epoch - 34ms/step\n",
            "Epoch 5/25\n",
            "841/841 - 28s - loss: 0.5603 - acc: 0.7007 - val_loss: 0.5669 - val_acc: 0.6909 - 28s/epoch - 34ms/step\n",
            "Epoch 6/25\n",
            "841/841 - 28s - loss: 0.5511 - acc: 0.7088 - val_loss: 0.5609 - val_acc: 0.7018 - 28s/epoch - 34ms/step\n",
            "Epoch 7/25\n",
            "841/841 - 28s - loss: 0.5426 - acc: 0.7174 - val_loss: 0.5556 - val_acc: 0.7041 - 28s/epoch - 34ms/step\n",
            "Epoch 8/25\n",
            "841/841 - 29s - loss: 0.5365 - acc: 0.7194 - val_loss: 0.5535 - val_acc: 0.7037 - 29s/epoch - 34ms/step\n",
            "Epoch 9/25\n",
            "841/841 - 28s - loss: 0.5268 - acc: 0.7254 - val_loss: 0.5533 - val_acc: 0.7080 - 28s/epoch - 34ms/step\n",
            "Epoch 10/25\n",
            "841/841 - 28s - loss: 0.5184 - acc: 0.7299 - val_loss: 0.5648 - val_acc: 0.7107 - 28s/epoch - 34ms/step\n",
            "Epoch 11/25\n",
            "841/841 - 29s - loss: 0.5142 - acc: 0.7321 - val_loss: 0.5603 - val_acc: 0.7089 - 29s/epoch - 34ms/step\n",
            "Epoch 12/25\n",
            "841/841 - 29s - loss: 0.5067 - acc: 0.7357 - val_loss: 0.5512 - val_acc: 0.7095 - 29s/epoch - 34ms/step\n",
            "Epoch 13/25\n",
            "841/841 - 28s - loss: 0.5045 - acc: 0.7403 - val_loss: 0.5574 - val_acc: 0.7107 - 28s/epoch - 34ms/step\n",
            "Epoch 14/25\n",
            "841/841 - 29s - loss: 0.4977 - acc: 0.7441 - val_loss: 0.5589 - val_acc: 0.7134 - 29s/epoch - 34ms/step\n",
            "Epoch 15/25\n",
            "841/841 - 29s - loss: 0.4916 - acc: 0.7492 - val_loss: 0.5686 - val_acc: 0.7036 - 29s/epoch - 34ms/step\n",
            "Epoch 16/25\n",
            "841/841 - 28s - loss: 0.4884 - acc: 0.7483 - val_loss: 0.5583 - val_acc: 0.7110 - 28s/epoch - 34ms/step\n",
            "Epoch 17/25\n",
            "841/841 - 29s - loss: 0.4817 - acc: 0.7554 - val_loss: 0.5581 - val_acc: 0.7047 - 29s/epoch - 34ms/step\n",
            "Epoch 18/25\n",
            "841/841 - 28s - loss: 0.4792 - acc: 0.7566 - val_loss: 0.5620 - val_acc: 0.7074 - 28s/epoch - 34ms/step\n",
            "Epoch 19/25\n",
            "841/841 - 29s - loss: 0.4740 - acc: 0.7596 - val_loss: 0.5748 - val_acc: 0.7123 - 29s/epoch - 34ms/step\n",
            "Epoch 20/25\n",
            "841/841 - 29s - loss: 0.4689 - acc: 0.7636 - val_loss: 0.5677 - val_acc: 0.7108 - 29s/epoch - 34ms/step\n",
            "Epoch 21/25\n",
            "841/841 - 29s - loss: 0.4659 - acc: 0.7653 - val_loss: 0.5778 - val_acc: 0.7082 - 29s/epoch - 34ms/step\n",
            "Epoch 22/25\n",
            "841/841 - 29s - loss: 0.4611 - acc: 0.7690 - val_loss: 0.5855 - val_acc: 0.7091 - 29s/epoch - 34ms/step\n",
            "Epoch 23/25\n",
            "841/841 - 29s - loss: 0.4581 - acc: 0.7679 - val_loss: 0.5810 - val_acc: 0.7098 - 29s/epoch - 34ms/step\n",
            "Epoch 24/25\n",
            "841/841 - 29s - loss: 0.4548 - acc: 0.7683 - val_loss: 0.5783 - val_acc: 0.7047 - 29s/epoch - 34ms/step\n",
            "Epoch 25/25\n",
            "841/841 - 29s - loss: 0.4515 - acc: 0.7726 - val_loss: 0.5824 - val_acc: 0.7114 - 29s/epoch - 34ms/step\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(X_train_pad, y_train, batch_size=32, epochs=25, validation_data=(X_test_pad, y_test), verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0uecE9adGm-"
      },
      "outputs": [],
      "source": [
        "def predict_sarcasm(s):\n",
        "    x_final = pd.DataFrame({\"headline\":[s]})\n",
        "    test_lines = CleanTokenize(x_final)\n",
        "    test_sequences = tokenizer_obj.texts_to_sequences(test_lines)\n",
        "    test_review_pad = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n",
        "    pred = model.predict(test_review_pad)\n",
        "    pred*=100\n",
        "    if pred[0][0]>=50: return \"It's a sarcasm!\" \n",
        "    else: return \"It's not a sarcasm.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZPIl-HsldMTx",
        "outputId": "25f3f9c9-e687-45a1-d2e7-788854193ee4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"It's a sarcasm!\""
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_sarcasm(\"I was depressed. He asked me to be happy. I am not depressed anymore.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VjgI86SldTHq",
        "outputId": "ebb55004-13be-4b79-eaf5-187ab9a04e3c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"It's a sarcasm!\""
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_sarcasm(\"You just broke my car window. Great job.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kwgiiMMPdXcX",
        "outputId": "e22b4611-74c9-47a2-80c9-64ff68156cd8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"It's not a sarcasm.\""
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_sarcasm(\"I want a million dollars!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xxZ-4753daog",
        "outputId": "8a646be4-b9af-441c-9a6c-4d1053e285a3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"It's a sarcasm!\""
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_sarcasm(\"I just won a million dollars!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MlgUYzb1ddHz",
        "outputId": "2c3c231c-3d41-4eec-a588-24df0ec87e5e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"It's a sarcasm!\""
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_sarcasm(\"I guess there is stupid everywhere\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "k-I9FqvHdiUx",
        "outputId": "3469383e-984c-4dbd-830c-f376d8cea099"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"It's not a sarcasm.\""
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_sarcasm(\"I’d love to discuss ideas and approaches to detect \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dPzDi0YmdoDo",
        "outputId": "dae5e88d-5fb1-4ff8-d41b-1da1a1d04198"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"It's a sarcasm!\""
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_sarcasm(\"qatar deporting dutch woman who reported she was drugged and raped\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "D10i-eqNeiQw",
        "outputId": "6b12e81c-629e-480e-c446-67a172afb742"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"It's a sarcasm!\""
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_sarcasm(\"you are stupid\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
